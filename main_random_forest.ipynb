{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T19:15:21.529530Z",
     "start_time": "2025-11-15T19:15:20.514517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "FILE_NAME = 'data/dataset.csv'\n",
    "TARGET_COLUMN = 'target_variable'\n",
    "RANDOM_STATE = 42\n",
    "# CHANGED: Test size to 20% (0.2) as requested\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "print(f\"--- Starting GTM ML Explainability Analysis (80/20 Split) ---\")\n",
    "\n",
    "# 1. Data Loading and Preparation\n",
    "try:\n",
    "    df = pd.read_csv(FILE_NAME)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{FILE_NAME}' was not found. Please ensure it's in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "# Drop the 'id' column as it's an identifier, not a feature\n",
    "X = df.drop(columns=['id', TARGET_COLUMN])\n",
    "y = df[TARGET_COLUMN]\n",
    "\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]} samples (80%)\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples (20%)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 2. Model Training: Random Forest Classifier\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1 # Use all available cores\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest Classifier...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Training complete.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 3. Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "\n",
    "print(\"## Model Performance Metrics (80/20 Split)\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}  <-- Key metric for business classification\")\n",
    "print(f\"ROC AUC:   {roc_auc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 4. Explainability (SHAP Analysis)\n",
    "\n",
    "print(\"## SHAP Analysis for Explainability\")"
   ],
   "id": "98ea924029e8da69",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/backo/Documents/datathon-2025/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting GTM ML Explainability Analysis (80/20 Split) ---\n",
      "\n",
      "Training set size: 28719 samples (80%)\n",
      "Testing set size: 7180 samples (20%)\n",
      "--------------------------------------------------\n",
      "Training Random Forest Classifier...\n",
      "Training complete.\n",
      "--------------------------------------------------\n",
      "## Model Performance Metrics (80/20 Split)\n",
      "Accuracy:  0.8436\n",
      "Precision: 0.8311\n",
      "Recall:    0.8378\n",
      "F1 Score:  0.8344  <-- Key metric for business classification\n",
      "ROC AUC:   0.9313\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85      3802\n",
      "           1       0.83      0.84      0.83      3378\n",
      "\n",
      "    accuracy                           0.84      7180\n",
      "   macro avg       0.84      0.84      0.84      7180\n",
      "weighted avg       0.84      0.84      0.84      7180\n",
      "\n",
      "--------------------------------------------------\n",
      "## SHAP Analysis for Explainability\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-15T19:15:21.614756Z",
     "start_time": "2025-11-15T19:15:21.547624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "joblib.dump(model, 'rootForest.pkl')\n",
    "print('Model saved to rootForest.pkl')"
   ],
   "id": "2ebd7b9f88d8b1da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to rootForest.pkl\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-15T19:15:21.633738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use shap.Explainer for tree-based models\n",
    "explainer = shap.Explainer(model)\n",
    "\n",
    "# Calculate SHAP values on a sample of the test data for faster computation\n",
    "# Using a sample size up to 5000 from the test set.\n",
    "X_sample = X_test.sample(n=min(5000, X_test.shape[0]), random_state=RANDOM_STATE)\n",
    "shap_values = explainer(X_sample)\n",
    "\n",
    "input()\n",
    "\n",
    "# --- Plot 1: SHAP Summary Plot (Feature Importance and Impact) ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "# This plot summarizes feature importance globally.\n",
    "shap.summary_plot(shap_values, X_sample, show=False)\n",
    "plt.title(\"SHAP Summary Plot (80/20 Split)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "summary_plot_path = 'rf_shap_summary_plot_80_20.png'\n",
    "plt.savefig(summary_plot_path, bbox_inches='tight')\n",
    "print(f\"SHAP Summary Plot saved to: {summary_plot_path}\")\n",
    "plt.close()\n",
    "input()\n",
    "# --- Plot 2: SHAP Dependence Plot (Feature Interaction) ---\n",
    "\n",
    "# Get the feature with the highest mean absolute SHAP value\n",
    "shap_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'mean_abs_shap': np.abs(shap_values.values).mean(axis=0)\n",
    "}).sort_values(by='mean_abs_shap', ascending=False)\n",
    "\n",
    "most_important_feature = shap_df.iloc[0]['feature']\n",
    "print(f\"Most important feature (for Dependence Plot): {most_important_feature}\")\n",
    "\n",
    "# This plot shows how the most important feature affects the prediction.\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.dependence_plot(\n",
    "    most_important_feature,\n",
    "    shap_values.values,\n",
    "    X_sample,\n",
    "    interaction_index='auto', # Automatically finds the best feature to show interaction\n",
    "    show=False\n",
    ")\n",
    "plt.title(f\"SHAP Dependence Plot for: {most_important_feature} (80/20 Split)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "dependence_plot_path = 'rf_shap_dependence_plot_80_20.png'\n",
    "plt.savefig(dependence_plot_path, bbox_inches='tight')\n",
    "print(f\"SHAP Dependence Plot saved to: {dependence_plot_path}\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n--- Script finished successfully ---\")"
   ],
   "id": "36617b9a184cfda0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ebb991dffc1478ee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
